\documentclass{beamer}

\input{settings.tex}


\title{Optimization problems, Analytic solutions}
\subtitle{Computational Intelligence, Lecture 3}
\author{by Sergei Savin}
\centering
\date{\mydate}



\begin{document}
\maketitle


\begin{frame}{Content}

\begin{itemize}
\item Optimization problem
\item Feasibility problem
\item Norms and quadratic forms
\item Problems with analytical solutions
\item Weighted pseudoinverse
\end{itemize}

\end{frame}




\begin{frame}{Optimization problem}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		An optimization problem has the following form:
		%
		\begin{equation}
			\begin{aligned}
				& \underset{\text{decision variables}}{\text{minimize}}
				& & \text{cost function}, \\
				& \text{subject to}
				& & \text{constraints}.
			\end{aligned}
		\end{equation}
	
		Where the solution to the optimization problem is the optimal value of the decision variables.
		
		\bigskip
		
		For example:
		%
		\begin{equation}
			\begin{aligned}
				& \underset{\bo{x}}{\text{minimize}}
				& & f(\bo{x}), \\
				& \text{subject to}
				& & \begin{cases}
					g(\bo{x}) = 0, \\
					h(\bo{x}) \leq 0.
				\end{cases}
			\end{aligned}
		\end{equation}
	
	In this example, $\bo{x} \in \R^n$ is the decision variable, $f(\bo{x}): \ \R^n \rightarrow \R$ is a cost function, $g(\bo{x}) = 0$ are equality constraints, and $h(\bo{x}) \leq 0$ are inequality constraints.

	\end{flushleft}
\end{frame}



\begin{frame}{Feasibility problem}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		A cost function is always scalar. A special case of a cost function is a constant:
		%
		\begin{equation}
			\begin{aligned}
				& \underset{\bo{x}}{\text{minimize}}
				& & 0, \\
				& \text{subject to}
				& & \begin{cases}
					g(\bo{x}) = 0, \\
					h(\bo{x}) \leq 0.
				\end{cases}
			\end{aligned}
		\end{equation}
	
		In this case any $\bo{x}$ that satisfies constraints would be a solution to the problem. It is called a \emph{feasibility problem}. We solved this type of problems to find out if there exist any $\bo{x}$ that satisfies constraints.
		
	\end{flushleft}
\end{frame}




\begin{frame}{Unconstrained optimization}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		Often an optimization problem would not feature constraints:
		%
		\begin{equation}
			\begin{aligned}
				& \underset{\bo{x}}{\text{minimize}}
				& & f(\bo{x})
			\end{aligned}
		\end{equation}
		
		We can call it \emph{unconstrained optimization}.
		
		\bigskip
		
		Note that the decision variable $\bo{x}$ can belong to a set $\bo{x} \in \mathcal{X}$ or the cost function may have a domain $f: \ \mathcal{D} \rightarrow \R$; in these cases, the set of allowed values of $\bo{x}$, as well as the domain of the function represent \emph{implicit} constraints. 
		
		\bigskip
		
		For example, the problem:
		
		$$\underset{x}{\text{minimize}} \ \ \ln x$$
		
		has an implicit constraint $x \geq 0$.
		
	\end{flushleft}
\end{frame}


\begin{frame}{Problems with analytical solutions}
% \framesubtitle{Parameter estimation}
\begin{flushleft}
	

Some types of optimization problems admit an analytic solution. For example:
	

Problem 1. $\text{minimize} \ ||\mathbf{x}||$. \textcolor{white}{Solution $\mathbf{x} = \mathbf{0}$} 

\bigskip

Problem 2. $\text{minimize} \ ||\mathbf{A}\mathbf{x}||$. \textcolor{white}{Solution $\mathbf{x} = \mathbf{0}$.}

\bigskip

Problem 3. $\text{minimize} \ ||\mathbf{A}\mathbf{x} + \mathbf{b}||$. 

\bigskip

We know solution of $\textcolor{mydarkgray}{\text{minimize} \ ||\mathbf{A}\mathbf{x} - \mathbf{b}||}$, which is $\mathbf{x} = \mathbf{A}^+ \mathbf{b}$. Therefore the problem 3 has a solution $\mathbf{x} = -\mathbf{A}^+ \mathbf{b}$.
 
\end{flushleft}
\end{frame}





\begin{frame}{Norms and quadratic forms}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		
		Note that the following problems will always have the same solutions:
		
		\begin{itemize}
			\item $\text{minimize} \ ||\mathbf{A}\mathbf{x} + \mathbf{b}||$;
			
			\item $\text{minimize} \ (\mathbf{A}\mathbf{x} + \mathbf{b})\T (\mathbf{A}\mathbf{x} + \mathbf{b})$;
		\end{itemize}
	
		This is because square root is a monotonic function.
		
		\bigskip
		
		This \textbf{does not} imply equivalence of the following problems:
		
		\begin{itemize}
			\item $\text{minimize} \ \sum ||\mathbf{A}_i\mathbf{x} + \mathbf{b}_i||$;
			
			\item $\text{minimize} \  \sum (\mathbf{A}_i\mathbf{x} + \mathbf{b}_i)\T (\mathbf{A}_i\mathbf{x} + \mathbf{b}_i)$;
		\end{itemize}
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Problems with analytical solutions}
%\framesubtitle{Part 3}
\begin{flushleft}


Problem 4. 
%
\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & || \mathbf{x} ||, \\
& \text{subject to}
& & \mathbf{A} \mathbf{x} = \mathbf{c}.
\end{aligned}
\end{equation}


All solutions to $\mathbf{A} \mathbf{x} = \mathbf{c}$ are written as $\mathbf{x} = \mathbf{A}^+\mathbf{c} + \mathbf{N}\mathbf{z}$, where $\mathbf{N} = \text{null}(\mathbf{A})$, and $\mathbf{A}^+\mathbf{c} \in \text{row}(\mathbf{A})$ as we proved previously. Since null space solution $\mathbf{N}\mathbf{z}$ and row space paricular solution $\mathbf{A}^+\mathbf{c}$ are orthagonal, the minimum norm solution corresponds to $\mathbf{z} = \mathbf{0}$, hence $\mathbf{x} = \mathbf{A}^+\mathbf{c}$.



\end{flushleft}
\end{frame}



\begin{frame}{Problems with analytical solutions}
%\framesubtitle{Part 4}
\begin{flushleft}


Thus, the solution is  $\mathbf{x} = \mathbf{A}^+\mathbf{c}$. Notice that solutions for the problem 4 and problem 3 are written identically (sans the sign), even though problem 3 asks us to minimize residual of the linear system, while problem 4 - find minimum norm solution. 

\bigskip

This illustrates an important fact that solution to the least squares problem, formulated either as "minimization of a residual" or as a "minimum norm solution" are given by the same formula, which we call Moore-Penrose pseudoinverse.


\end{flushleft}
\end{frame}





\begin{frame}{Problems with analytical solutions}
%\framesubtitle{Part 4}
\begin{flushleft}

Problem 5. 
%
\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & || \mathbf{D}\mathbf{x} ||, \\
& \text{subject to}
& & \mathbf{A} \mathbf{x} = \mathbf{b}.
\end{aligned}
\end{equation}

One way to think about it is to first find all solution to the constraint equation $\mathbf{A} \mathbf{x} = \mathbf{b}$ and then find optimal one among them. As we know, all solutions are given as: $\mathbf{x} = \mathbf{A}^+\mathbf{b} + \mathbf{N}\mathbf{z}$, where $\mathbf{N} = \text{null}(\mathbf{A})$. Then our cost function becomes: $|| \mathbf{D}\mathbf{A}^+\mathbf{b} +  \mathbf{D}\mathbf{N}\mathbf{z} ||$, which is equivalent to the problem 3. Thus, we can write solution as: 
$\mathbf{z}^* = -(\mathbf{D}\mathbf{N})^+ \mathbf{D}\mathbf{A}^+\mathbf{b}$. In terms of $\mathbf{x}$ solution is:

\begin{equation}
    \mathbf{x}^* = \mathbf{A}^+\mathbf{b}-\mathbf{N}(\mathbf{D}\mathbf{N})^+ \mathbf{D}\mathbf{A}^+\mathbf{b}
\end{equation}
\begin{equation}
	\mathbf{x}^* =( \mathbf{I}-\mathbf{N}(\mathbf{D}\mathbf{N})^+ \mathbf{D})\mathbf{A}^+\mathbf{b}
\end{equation}

\end{flushleft}
\end{frame}



\begin{frame}{Problems with analytical solutions}
%\framesubtitle{Part 5}
\begin{flushleft}

Problem 6. 
%
\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & || \mathbf{D}\mathbf{x} + \mathbf{f} ||, \\
& \text{subject to}
& & \mathbf{A} \mathbf{x} = \mathbf{b}.
\end{aligned}
\end{equation}

After the same initial step, we arrive at the cost function $|| \mathbf{D}\mathbf{N}\mathbf{z} + \mathbf{D}\mathbf{A}^+\mathbf{b} + \mathbf{f}||$. It is only different in the constant term, and the solution is found as follows:


\begin{equation}
    \mathbf{z}^* = -(\mathbf{D}\mathbf{N})^+ (\mathbf{D}\mathbf{A}^+\mathbf{b} + \mathbf{f})
\end{equation}
\begin{equation}
    \mathbf{x}^* = \mathbf{A}^+\mathbf{b}-\mathbf{N}(\mathbf{D}\mathbf{N})^+ (\mathbf{D}\mathbf{A}^+\mathbf{b} + \mathbf{f})
\end{equation}


\end{flushleft}
\end{frame}



\begin{frame}{Problems with analytical solutions}
%\framesubtitle{Part 6}
\begin{flushleft}

Problem 7. 
%
\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & \mathbf{x}^\top \mathbf{H} \mathbf{x} + \mathbf{c}^\top\mathbf{x}, \\
& \text{subject to}
& & \mathbf{A} \mathbf{x} = \mathbf{b}.
\end{aligned}
\end{equation}

where $\mathbf{H}$ is positive-definite.

\bigskip

Assume that we found a decomposition $\mathbf{H} = \mathbf{D}^\top\mathbf{D}$. We can also find such $\mathbf{f}$ that $2\mathbf{f}^\top\mathbf{D} = \mathbf{c}^\top$. Then our cost function becomes $\mathbf{x}^\top \mathbf{D}^\top\mathbf{D} \mathbf{x} + 2\mathbf{f}^\top\mathbf{D}\mathbf{x}$, which as we saw before has coinciding minimum with the cost function $||\mathbf{D}\mathbf{x} + \mathbf{f}||$.

\bigskip

Therefore the problem has the same solution as Problem 5, after the mentioned above change in constants.

\end{flushleft}
\end{frame}



\begin{frame}{Weighted pseudoinverse, unconstrained type}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		Consider a weighted pseudoinverse problem:
		%
		\begin{align}
			\text{minimize} \ \ ||\bo{A}\bo{x}-\bo{b}||_\bo{W}
		\end{align}		
		%
		where $||\bo{x}||_\bo{W} = \sqrt{\bo{x}\T\bo{W}\bo{x}}$ and $\bo{W} > 0$. We can re-write the problem as:
		%
		\begin{align}
			\text{minimize} \ \ (\bo{A}\bo{x}-\bo{b})\T\bo{W}^\frac{1}{2}\bo{W}^\frac{1}{2}(\bo{A}\bo{x}-\bo{b})
		\end{align}
		
		But this is the same as solving least-squares problem for equality $\bo{W}^\frac{1}{2}\bo{A}\bo{x}=\bo{W}^\frac{1}{2}\bo{b}$, which is does via Moore-Penrose pseudoinverse:
		%
		\begin{align}
			\bo{x}=(\bo{W}^\frac{1}{2}\bo{A})^+\bo{W}^\frac{1}{2}\bo{b}
		\end{align}
		
	\end{flushleft}
\end{frame}


\begin{frame}{Weighted pseudoinverse, constrained type}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		Consider a weighted pseudoinverse problem:
		%
		\begin{equation}
			\begin{aligned}
				& \underset{\bo{x}}{\text{minimize}}
				& & \bo{x}\T \bo{W} \bo{x}, \\
				& \text{subject to}
				& & \bo{A}\bo{x}=\bo{b}
			\end{aligned}
		\end{equation}
		
		We can use Lagrange multipliers to rewrite the problem as minimization of the function $L(\bo{x}, \lambda) = \bo{x}\T \bo{W} \bo{x} + \lambda\T ( \bo{A}\bo{x}-\bo{b})$; optimality conditions imply that $\frac{\partial L}{\partial  \bo{x}} = 0$ and $\frac{\partial  L}{\partial  \lambda} = \bo{A}\bo{x}-\bo{b} = 0$, so:
		%
		\begin{align}
			2\bo{x}\T \bo{W} + \lambda\T \bo{A} = 0
		\end{align}		
		
		This implies $\bo{x} = \frac{1}{2} \bo{W}^{-1} \bo{A}\T \lambda$, and since $\bo{A}\bo{x}-\bo{b} = 0$, we get:
		%
		\begin{align}
			\frac{1}{2}\bo{A}\bo{W}^{-1} \bo{A}\T \lambda= \bo{b} \\
			\lambda = 2 (\bo{A}\bo{W}^{-1} \bo{A}\T)^+ \bo{b} \\
			\bo{x} = \bo{W}^{-1} \bo{A}\T (\bo{A}\bo{W}^{-1} \bo{A}\T)^+ \bo{b}
		\end{align}		
		
		
	\end{flushleft}
\end{frame}







\myqrframe

\end{document}
